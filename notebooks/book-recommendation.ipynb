{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40915b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71d34de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b81f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 18:33:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"book-recs\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02af5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('location', StringType(), True), \n",
    "    StructField('age', FloatType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])\n",
    "\n",
    "users_df = spark.read.csv(\n",
    "    path='data/Users.csv', \n",
    "    schema=users_schema,\n",
    "    mode='PERMISSIVE',\n",
    "    columnNameOfCorruptRecord='_corrupt_record',\n",
    "    escape='\"'\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932e1a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------------------+\n",
      "|user_id|  location| age|     _corrupt_record|\n",
      "+-------+----------+----+--------------------+\n",
      "|   null|  Location|null|User-ID,Location,Age|\n",
      "| 275081|cernusco s|null|  275081,\"cernusco s|\n",
      "|   null|     milan|null|    , milan, italy\",|\n",
      "+-------+----------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "corrupt_user_records = users_df.filter(~users_df._corrupt_record.isNull())\n",
    "corrupt_user_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a67c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrupt records to drop: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of corrupt records to drop: {corrupt_user_records.count()}')\n",
    "users_df = users_df.filter(users_df._corrupt_record.isNull())\n",
    "users_df = users_df.drop('_corrupt_record')\n",
    "users_df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67714867",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_schema = StructType([\n",
    "    StructField('isbn', StringType(), True),\n",
    "    StructField('book_title', StringType(), True), \n",
    "    StructField('book_author', StringType(), True),\n",
    "    StructField('year_of_publication', IntegerType(), True),\n",
    "    StructField('publisher', StringType(), True),\n",
    "    StructField('image_url_s', StringType(), True),\n",
    "    StructField('image_url_m', StringType(), True),\n",
    "    StructField('image_url_l', StringType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])\n",
    "\n",
    "books_df = spark.read.csv(\n",
    "    path='data/Books.csv', \n",
    "    schema=books_schema,\n",
    "    mode='PERMISSIVE',\n",
    "    columnNameOfCorruptRecord='_corrupt_record',\n",
    "    escape='\"',\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58e79c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------------------+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|      isbn|          book_title|book_author|year_of_publication|           publisher|         image_url_s|         image_url_m|image_url_l|     _corrupt_record|\n",
      "+----------+--------------------+-----------+-------------------+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|      ISBN|          Book-Title|Book-Author|               null|           Publisher|         Image-URL-S|         Image-URL-M|Image-URL-L|ISBN,Book-Title,B...|\n",
      "|078946697X|DK Readers: Creat...|       2000|               null|http://images.ama...|http://images.ama...|http://images.ama...|       null|078946697X,\"DK Re...|\n",
      "|2070426769|Peuple du ciel, s...|       2003|               null|http://images.ama...|http://images.ama...|http://images.ama...|       null|2070426769,\"Peupl...|\n",
      "|0789466953|DK Readers: Creat...|       2000|               null|http://images.ama...|http://images.ama...|http://images.ama...|       null|0789466953,\"DK Re...|\n",
      "+----------+--------------------+-----------+-------------------+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "corrupt_book_records = books_df.filter(~books_df._corrupt_record.isNull())\n",
    "corrupt_book_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9184189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrupt records to drop: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of corrupt records to drop: {corrupt_book_records.count()}')\n",
    "books_df = books_df.filter(books_df._corrupt_record.isNull())\n",
    "books_df = books_df.drop('_corrupt_record')\n",
    "books_df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc19dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('isbn', StringType(), True), \n",
    "    StructField('book_rating', IntegerType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])\n",
    "\n",
    "ratings_df = spark.read.csv(\n",
    "    path='data/Ratings.csv', \n",
    "    schema=ratings_schema,\n",
    "    mode='PERMISSIVE',\n",
    "    columnNameOfCorruptRecord='_corrupt_record',\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a8176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------+--------------------+\n",
      "|user_id|isbn|book_rating|     _corrupt_record|\n",
      "+-------+----+-----------+--------------------+\n",
      "|   null|ISBN|       null|User-ID,ISBN,Book...|\n",
      "+-------+----+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "corrupt_rating_records = ratings_df.filter(~ratings_df._corrupt_record.isNull())\n",
    "corrupt_rating_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "654de7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrupt records to drop: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of corrupt records to drop: {corrupt_rating_records.count()}')\n",
    "ratings_df = ratings_df.filter(ratings_df._corrupt_record.isNull())\n",
    "ratings_df = ratings_df.drop('_corrupt_record')\n",
    "ratings_df.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2814607",
   "metadata": {},
   "source": [
    "First let's check if there are duplicate values in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b917ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupliactes in users_df: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupliactes in books_df: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=================================================>    (185 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupliactes in ratings_df: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'Dupliactes in users_df: {users_df.distinct().count() != users_df.count()}')\n",
    "print(f'Dupliactes in books_df: {books_df.distinct().count() != books_df.count()}')\n",
    "print(f'Dupliactes in ratings_df: {ratings_df.distinct().count() != ratings_df.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c3a48",
   "metadata": {},
   "source": [
    "Now let's count missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c729bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|location|   age|\n",
      "+-------+--------+------+\n",
      "|      0|       0|110761|\n",
      "+-------+--------+------+\n",
      "\n",
      "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
      "|isbn|book_title|book_author|year_of_publication|publisher|image_url_s|image_url_m|image_url_l|\n",
      "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
      "|   0|         0|          1|                  0|        2|          0|          0|          0|\n",
      "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
      "\n",
      "+-------+----+-----------+\n",
      "|user_id|isbn|book_rating|\n",
      "+-------+----+-----------+\n",
      "|      0|   0|          0|\n",
      "+-------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "for df in users_df, books_df, ratings_df:\n",
    "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd0f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      isbn|          book_title|book_author|year_of_publication|           publisher|         image_url_s|         image_url_m|         image_url_l|\n",
      "+----------+--------------------+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|9627982032|The Credit Suisse...|       null|               1995|Edinburgh Financi...|http://images.ama...|http://images.ama...|http://images.ama...|\n",
      "+----------+--------------------+-----------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.filter(books_df['book_author'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc19d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_book_author_isbns = books_df.filter(books_df['book_author'].isNull()).select('isbn').collect()\n",
    "ratings_df.filter(ratings_df.isbn.isin([row[0] for row in no_book_author_isbns])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969b486",
   "metadata": {},
   "source": [
    "Book with book_author missing was rated once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5f9df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "|      isbn|     book_title|    book_author|year_of_publication|publisher|         image_url_s|         image_url_m|         image_url_l|\n",
      "+----------+---------------+---------------+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "|193169656X|    Tyrant Moon|Elaine Corvidae|               2002|     null|http://images.ama...|http://images.ama...|http://images.ama...|\n",
      "|1931696993|Finders Keepers|Linnea Sinclair|               2001|     null|http://images.ama...|http://images.ama...|http://images.ama...|\n",
      "+----------+---------------+---------------+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.filter(books_df['publisher'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2371f1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_publisher_isbns = books_df.filter(books_df['publisher'].isNull()).select('isbn').collect()\n",
    "ratings_df.filter(ratings_df.isbn.isin([row[0] for row in no_publisher_isbns])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bff505",
   "metadata": {},
   "source": [
    "Books with publisher missing were rated a total of two times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "376de559",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df.na.fill('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f852d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df.drop('image_url_s', 'image_url_m', 'image_url_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82ddb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct isbn values in books_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(isbn)|\n",
      "+-----------+\n",
      "|     271357|\n",
      "+-----------+\n",
      "\n",
      "Distinct isbn values in ratings_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:====================================================> (196 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(isbn)|\n",
      "+-----------+\n",
      "|     340556|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "print('Distinct isbn values in books_df:')\n",
    "books_df.agg(countDistinct(col(\"isbn\"))).show()\n",
    "\n",
    "print('Distinct isbn values in ratings_df:')\n",
    "ratings_df.agg(countDistinct(col(\"isbn\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95e833",
   "metadata": {},
   "source": [
    "# Popularity-based recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b52cadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings_df.join(users_df, on='user_id', how='left')\n",
    "df = df.join(books_df, on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635625b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----------+--------+------+----------+-----------+-------------------+---------+\n",
      "|isbn|user_id|book_rating|location|   age|book_title|book_author|year_of_publication|publisher|\n",
      "+----+-------+-----------+--------+------+----------+-----------+-------------------+---------+\n",
      "|   0|      0|          0|       0|309492|    118648|     118648|             118648|   118648|\n",
      "+----+-------+-----------+--------+------+----------+-----------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2693fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=====================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------+\n",
      "|popularity|      isbn|          book_title|   book_author|\n",
      "+----------+----------+--------------------+--------------+\n",
      "|      2502|0971880107|         Wild Animus|  Rich Shapero|\n",
      "|      1295|0316666343|The Lovely Bones:...|  Alice Sebold|\n",
      "|       883|0385504209|   The Da Vinci Code|     Dan Brown|\n",
      "|       732|0060928336|Divine Secrets of...| Rebecca Wells|\n",
      "|       723|0312195516|The Red Tent (Bes...| Anita Diamant|\n",
      "|       647|044023722X|     A Painted House|  John Grisham|\n",
      "|       639|0679781587|                null|          null|\n",
      "|       615|0142001740|The Secret Life o...| Sue Monk Kidd|\n",
      "|       614|067976402X|Snow Falling on C...|David Guterson|\n",
      "|       586|0671027360| Angels &amp; Demons|     Dan Brown|\n",
      "+----------+----------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class PopularityBasedRecSys:\n",
    "    \n",
    "    def __init__(self, n_recs=5):\n",
    "        self.n_recs = n_recs\n",
    "        self.recs = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "#         self.recs = df.\\\n",
    "#                     groupBy('isbn').agg(count('isbn').alias('popularity')).\\\n",
    "#                     orderBy('popularity', ascending=False)\n",
    "        df.createOrReplaceTempView('data')\n",
    "        self.recs = spark.sql('''SELECT COUNT(isbn) AS popularity, isbn, book_title, book_author\n",
    "                                 FROM data\n",
    "                                 GROUP BY isbn, book_title, book_author\n",
    "                                 ORDER BY COUNT(isbn) DESC''')\n",
    "       \n",
    "    def predict(self):\n",
    "        return self.recs.limit(self.n_recs)\n",
    "\n",
    "\n",
    "pop_recsys = PopularityBasedRecSys(n_recs=10)\n",
    "pop_recsys.fit(df)\n",
    "book_recs = pop_recsys.predict()\n",
    "book_recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cebd325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------+-------------------+---------+\n",
      "|isbn|book_title|book_author|year_of_publication|publisher|\n",
      "+----+----------+-----------+-------------------+---------+\n",
      "+----+----------+-----------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.filter(books_df.isbn == '0679781587').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeba57a",
   "metadata": {},
   "source": [
    "Since 0679781587 isbn is not present in books_df, book_title and book_author will not be present in the resulting data frame of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf451afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------------+\n",
      "|      isbn|popularity|          book_title|     book_author|\n",
      "+----------+----------+--------------------+----------------+\n",
      "|0439064864|      6.61|Harry Potter and ...|   J. K. Rowling|\n",
      "|0439139597|      6.54|Harry Potter and ...|   J. K. Rowling|\n",
      "|0439136350|      6.47|Harry Potter and ...|   J. K. Rowling|\n",
      "|0590353403|      6.36|Harry Potter and ...|   J. K. Rowling|\n",
      "|043935806X|      5.57|Harry Potter and ...|   J. K. Rowling|\n",
      "|0439136369|      5.35|Harry Potter and ...|   J. K. Rowling|\n",
      "|0812550706|       5.3|Ender's Game (End...|Orson Scott Card|\n",
      "|0671027344|      5.19|The Perks of Bein...| Stephen Chbosky|\n",
      "|0439139600|       5.1|Harry Potter and ...|   J. K. Rowling|\n",
      "|0345339681|      5.01|The Hobbit : The ...|  J.R.R. TOLKIEN|\n",
      "|0446310786|      4.92|To Kill a Mocking...|      Harper Lee|\n",
      "|0440219078|      4.92|The Giver (21st C...|      LOIS LOWRY|\n",
      "|0553375407|      4.91|Ishmael: An Adven...|    Daniel Quinn|\n",
      "|059035342X|       4.9|Harry Potter and ...|   J. K. Rowling|\n",
      "|039592720X|      4.87|Interpreter of Ma...|   Jhumpa Lahiri|\n",
      "+----------+----------+--------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class HighestRatedPopularityBasedRecSys(PopularityBasedRecSys):\n",
    "    \n",
    "    def __init__(self, min_num_ratings=100, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_num_ratings = min_num_ratings\n",
    "        \n",
    "    def fit(self, df):\n",
    "        df.createOrReplaceTempView('data')\n",
    "        self.recs = spark.sql(f'''SELECT isbn, ROUND(AVG(book_rating), 2) AS popularity, book_title, book_author\n",
    "                                  FROM data\n",
    "                                  GROUP BY isbn, book_title, book_author\n",
    "                                  HAVING COUNT(isbn) > {self.min_num_ratings}\n",
    "                                  ORDER BY popularity DESC''')\n",
    "        \n",
    "        \n",
    "highest_rated_pop_recsys = HighestRatedPopularityBasedRecSys(n_recs=15)\n",
    "highest_rated_pop_recsys.fit(df)\n",
    "highest_rated_book_recs = highest_rated_pop_recsys.predict()\n",
    "highest_rated_book_recs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66966723",
   "metadata": {},
   "source": [
    "# Model-based collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61f55498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='isbn', outputCol='isbn_indexed')\n",
    "ratings_df = indexer.fit(ratings_df).transform(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6d8b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 18:35:02 WARN DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+------------+\n",
      "|user_id|      isbn|book_rating|isbn_indexed|\n",
      "+-------+----------+-----------+------------+\n",
      "| 276725|034545104X|          0|      1637.0|\n",
      "| 276726|0155061224|          5|     89066.0|\n",
      "| 276727|0446520802|          0|       568.0|\n",
      "| 276729|052165615X|          3|    205975.0|\n",
      "| 276729|0521795028|          6|    206005.0|\n",
      "+-------+----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf74773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "train, test = ratings_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "als = ALS(\n",
    "    userCol='user_id',\n",
    "    itemCol='isbn_indexed',\n",
    "    ratingCol='book_rating',\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy='drop'\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='book_rating')\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "         .addGrid(als.rank, [10, 20,])\\\n",
    "         .addGrid(als.maxIter, [10, 15, 20])\\\n",
    "         .addGrid(als.regParam, [0.01, 0.1, 0.5])\\\n",
    "         .build()\n",
    "         \n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=params,\n",
    "    evaluator=evaluator\n",
    ")\n",
    "\n",
    "model = tvs.fit(train)\n",
    "preds = model.transform(test)\n",
    "rmse = evaluator.evaluate(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
