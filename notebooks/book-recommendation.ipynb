{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd4783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60b3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab7bf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 18:33:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"book-recs\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f2d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('location', StringType(), True), \n",
    "    StructField('age', FloatType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])\n",
    "\n",
    "users_df = spark.read.csv(\n",
    "    path='data/Users.csv', \n",
    "    schema=users_schema,\n",
    "    mode='PERMISSIVE',\n",
    "    columnNameOfCorruptRecord='_corrupt_record',\n",
    "    escape='\"'\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "corrupt_user_records = users_df.filter(~users_df._corrupt_record.isNull())\n",
    "corrupt_user_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of corrupt records to drop: {corrupt_user_records.count()}')\n",
    "users_df = users_df.filter(users_df._corrupt_record.isNull())\n",
    "users_df = users_df.drop('_corrupt_record')\n",
    "users_df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_schema = StructType([\n",
    "    StructField('isbn', StringType(), True),\n",
    "    StructField('book_title', StringType(), True), \n",
    "    StructField('book_author', StringType(), True),\n",
    "    StructField('year_of_publication', IntegerType(), True),\n",
    "    StructField('publisher', StringType(), True),\n",
    "    StructField('image_url_s', StringType(), True),\n",
    "    StructField('image_url_m', StringType(), True),\n",
    "    StructField('image_url_l', StringType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])\n",
    "\n",
    "books_df = spark.read.csv(\n",
    "    path='data/Books.csv', \n",
    "    schema=books_schema,\n",
    "    mode='PERMISSIVE',\n",
    "    columnNameOfCorruptRecord='_corrupt_record',\n",
    "    escape='\"',\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_book_records = books_df.filter(~books_df._corrupt_record.isNull())\n",
    "corrupt_book_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of corrupt records to drop: {corrupt_book_records.count()}')\n",
    "books_df = books_df.filter(books_df._corrupt_record.isNull())\n",
    "books_df = books_df.drop('_corrupt_record')\n",
    "books_df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('isbn', StringType(), True), \n",
    "    StructField('book_rating', IntegerType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])\n",
    "\n",
    "ratings_df = spark.read.csv(\n",
    "    path='data/Ratings.csv', \n",
    "    schema=ratings_schema,\n",
    "    mode='PERMISSIVE',\n",
    "    columnNameOfCorruptRecord='_corrupt_record',\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69392d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_rating_records = ratings_df.filter(~ratings_df._corrupt_record.isNull())\n",
    "corrupt_rating_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a507778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of corrupt records to drop: {corrupt_rating_records.count()}')\n",
    "ratings_df = ratings_df.filter(ratings_df._corrupt_record.isNull())\n",
    "ratings_df = ratings_df.drop('_corrupt_record')\n",
    "ratings_df.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e178534",
   "metadata": {},
   "source": [
    "First let's check if there are duplicate values in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dupliactes in users_df: {users_df.distinct().count() != users_df.count()}')\n",
    "print(f'Dupliactes in books_df: {books_df.distinct().count() != books_df.count()}')\n",
    "print(f'Dupliactes in ratings_df: {ratings_df.distinct().count() != ratings_df.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5928f",
   "metadata": {},
   "source": [
    "Now let's count missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "for df in users_df, books_df, ratings_df:\n",
    "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd47d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.filter(books_df['book_author'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_book_author_isbns = books_df.filter(books_df['book_author'].isNull()).select('isbn').collect()\n",
    "ratings_df.filter(ratings_df.isbn.isin([row[0] for row in no_book_author_isbns])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fc37e",
   "metadata": {},
   "source": [
    "Book with book_author missing was rated once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.filter(books_df['publisher'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c697822",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_publisher_isbns = books_df.filter(books_df['publisher'].isNull()).select('isbn').collect()\n",
    "ratings_df.filter(ratings_df.isbn.isin([row[0] for row in no_publisher_isbns])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76410a",
   "metadata": {},
   "source": [
    "Books with publisher missing were rated a total of two times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df.na.fill('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60258dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df.drop('image_url_s', 'image_url_m', 'image_url_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "print('Distinct isbn values in books_df:')\n",
    "books_df.agg(countDistinct(col(\"isbn\"))).show()\n",
    "\n",
    "print('Distinct isbn values in ratings_df:')\n",
    "ratings_df.agg(countDistinct(col(\"isbn\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b659792",
   "metadata": {},
   "source": [
    "# Popularity-based recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings_df.join(users_df, on='user_id', how='left')\n",
    "df = df.join(books_df, on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90578306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ccd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityBasedRecSys:\n",
    "    \n",
    "    def __init__(self, n_recs=5):\n",
    "        self.n_recs = n_recs\n",
    "        self.recs = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "#         self.recs = df.\\\n",
    "#                     groupBy('isbn').agg(count('isbn').alias('popularity')).\\\n",
    "#                     orderBy('popularity', ascending=False)\n",
    "        df.createOrReplaceTempView('data')\n",
    "        self.recs = spark.sql('''SELECT COUNT(isbn) AS popularity, isbn, book_title, book_author\n",
    "                                 FROM data\n",
    "                                 GROUP BY isbn, book_title, book_author\n",
    "                                 ORDER BY COUNT(isbn) DESC''')\n",
    "       \n",
    "    def predict(self):\n",
    "        return self.recs.limit(self.n_recs)\n",
    "\n",
    "\n",
    "pop_recsys = PopularityBasedRecSys(n_recs=10)\n",
    "pop_recsys.fit(df)\n",
    "book_recs = pop_recsys.predict()\n",
    "book_recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.filter(books_df.isbn == '0679781587').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73155c",
   "metadata": {},
   "source": [
    "Since 0679781587 isbn is not present in books_df, book_title and book_author will not be present in the resulting data frame of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64923a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighestRatedPopularityBasedRecSys(PopularityBasedRecSys):\n",
    "    \n",
    "    def __init__(self, min_num_ratings=100, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_num_ratings = min_num_ratings\n",
    "        \n",
    "    def fit(self, df):\n",
    "        df.createOrReplaceTempView('data')\n",
    "        self.recs = spark.sql(f'''SELECT isbn, ROUND(AVG(book_rating), 2) AS popularity, book_title, book_author\n",
    "                                  FROM data\n",
    "                                  GROUP BY isbn, book_title, book_author\n",
    "                                  HAVING COUNT(isbn) > {self.min_num_ratings}\n",
    "                                  ORDER BY popularity DESC''')\n",
    "        \n",
    "        \n",
    "highest_rated_pop_recsys = HighestRatedPopularityBasedRecSys(n_recs=15)\n",
    "highest_rated_pop_recsys.fit(df)\n",
    "highest_rated_book_recs = highest_rated_pop_recsys.predict()\n",
    "highest_rated_book_recs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd67f04",
   "metadata": {},
   "source": [
    "# Model-based collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbbbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='isbn', outputCol='isbn_indexed')\n",
    "ratings_df = indexer.fit(ratings_df).transform(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "train, test = ratings_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "als = ALS(\n",
    "    userCol='user_id',\n",
    "    itemCol='isbn_indexed',\n",
    "    ratingCol='book_rating',\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy='drop'\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='book_rating')\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "         .addGrid(als.rank, [10, 20,]).build()\n",
    "#          .addGrid(als.maxIter, [10, 15, 20])\\\n",
    "#          .addGrid(als.regParam, [0.01, 0.1, 0.5])\\\n",
    "         \n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=params,\n",
    "    evaluator=evaluator\n",
    ")\n",
    "\n",
    "model = tvs.fit(train)\n",
    "preds = model.transform(test)\n",
    "rmse = evaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a8f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f9481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
